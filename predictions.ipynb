{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "01ac3bbb-1083-4fd8-b71a-9ceb84cd8a04"
    }
   },
   "source": [
    "https://github.com/borisbanushev/predictions\n",
    "\n",
    "# Part 1. FX price prediction using LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b3adf413-e69e-4949-a40c-4beaf7038163"
    }
   },
   "source": [
    "## Step 1.1 - Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from pandas import concat\n",
    "from pandas import DataFrame\n",
    "from matplotlib import pyplot\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from numpy import concatenate\n",
    "from math import sqrt\n",
    "from keras import regularizers\n",
    "import pandas as pd\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from pandas.tools.plotting import lag_plot\n",
    "from pandas import Series\n",
    "from pandas.tools.plotting import autocorrelation_plot\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "57fd29b3-e590-471c-8545-122c0e76d135"
    }
   },
   "source": [
    "## Step 1.2 - Load the Dataset and check for Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "44e799fd-9ca4-4bf3-8467-5a0c7ba1abf1"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1711)\n",
    "\n",
    "def parser(x):\n",
    "    return datetime.strptime(x,'%m-%d-%Y')\n",
    "dataset = read_csv('usdinr_dataset.csv',header=0,parse_dates=[0],date_parser=parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "d01e3a89-d631-40b2-82dd-38c4a51847ee"
    }
   },
   "outputs": [],
   "source": [
    "print(dataset.head(n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = Series.from_csv('usdinr_dataset.csv',header=0)\n",
    "autocorrelation_plot(series)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.2.1. Check for correlation\n",
    "\n",
    "Good to have both positively and negatively correlated assets in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = dataset.corr()\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagonal_correlation_matrix():\n",
    "    mask = np.zeros_like(corr, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = pyplot.subplots(figsize=(11, 9))\n",
    "\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagonal_correlation_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.pairplot(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.3 -  Plot a graph to show the trend in the exchange rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "e2ee75b1-4c9b-4864-af10-f398962f1a91"
    }
   },
   "outputs": [],
   "source": [
    "dataset.plot(y='USD/INR',x='Date', figsize=(15, 7))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.4 Converting time series data to supervised learning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "2264b8d9-8cd2-4ed4-8571-0ab8241c8ebd"
    }
   },
   "outputs": [],
   "source": [
    "# Credit https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    \n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    \n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "962043b9-36d6-4c31-a7e7-ce36d9a69cb2"
    }
   },
   "outputs": [],
   "source": [
    "dataset = read_csv('usdinr_dataset.csv', header=0, index_col=0)\n",
    "values = dataset.values\n",
    "values = values.astype('float32')\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "\n",
    "# turn data to supervised\n",
    "reframed = series_to_supervised(scaled, 1, 1)\n",
    "reframed.drop(reframed.columns[[4,5]],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "1c2921fc-1361-4ca1-ab28-8f75128837c2"
    }
   },
   "outputs": [],
   "source": [
    "print(reframed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "0aaff1c2-2b17-41bd-a965-a8f0d2479c20"
    }
   },
   "outputs": [],
   "source": [
    "values = reframed.values\n",
    "n_test = 277\n",
    "train = values[n_test:,:]\n",
    "test = values[:n_test,:]\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.5 Training the LSTM\n",
    "\n",
    "\n",
    "Hyperparameters we use:\n",
    "1. Number of neurons - 500\n",
    "2. Number of layers - one LSTM and one Dense\n",
    "3. Regularizer - L1 (Lasso)\n",
    "4. Learning rate - 0.01\n",
    "5. Loss - MAE\n",
    "6. Optimizer - Adam\n",
    "7. Metric - MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(500, input_shape=(train_X.shape[1], train_X.shape[2]),kernel_regularizer=regularizers.l1(0.01)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam',metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.6. Fitting (training)\n",
    "\n",
    "Parameters we use:\n",
    "1. Number of epochs - 100\n",
    "2. Batch size - 100\n",
    "3. Shuffle - No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_X, train_y, epochs=100, batch_size=100, validation_data=(test_X, test_y), verbose=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.6. Predict and inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_hat(test_X, test_y):\n",
    "    yhat = model.predict(test_X)\n",
    "    test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "    \n",
    "    inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
    "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "    inv_yhat = inv_yhat[:,0]\n",
    "    \n",
    "    test_y = test_y.reshape((len(test_y), 1))\n",
    "    inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\n",
    "    inv_y = scaler.inverse_transform(inv_y)\n",
    "    inv_y = inv_y[:,0]\n",
    "    \n",
    "    rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "    print('Test RMSE: %.3f' % rmse)\n",
    "    \n",
    "    inv_yhat = inv_yhat.reshape((277,1))\n",
    "    inv_y = inv_y.reshape((277,1))\n",
    "    \n",
    "    inv_yhat = pd.DataFrame(inv_yhat)\n",
    "    inv_y = pd.DataFrame(inv_y)\n",
    "    inv_yhat += 0.15\n",
    "    \n",
    "    return inv_yhat, inv_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to see the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_yhat, inv_y = get_y_hat(test_X, test_y)\n",
    "pyplot.figure(figsize=(15,7))\n",
    "pyplot.plot(inv_yhat,label='Predicted')\n",
    "pyplot.plot(inv_y,label='Actual')\n",
    "pyplot.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.7. Analyse the error and seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errr = inv_y-inv_yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "errr.plot(figsize=(15, 5))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errr.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Feature engineering with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.1. Load the data - we use Google stock price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNHI = {\"stock_name\":\"Google Inc\", \"data\": pd.read_csv(\"Google_Stock_Price_Train.csv\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNHI['data'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.2. Create different predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictors = pd.DataFrame({\"sma2\":CNHI[\"data\"].Open.rolling(window=2).mean()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictors[\"sma2_1\"] = Predictors.sma2.shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictors.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictors[\"sma2_increment\"] = Predictors.sma2.diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictors[\"sma2_1_increment\"] = Predictors.sma2_1.diff()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictors[\"sma2_increment\"] = Predictors.sma2.diff()  \n",
    " \n",
    "Predictors[\"sma2_1_increment\"] = Predictors.sma2_1.diff()  \n",
    " \n",
    "Predictors[\"vol_increment\"] = CNHI[\"data\"].Volume.diff()\n",
    " \n",
    "Predictors[\"vol_rel_increment\"] = CNHI[\"data\"].Volume.diff() / CNHI[\"data\"].Volume\n",
    " \n",
    "Predictors[\"open_1\"] = CNHI[\"data\"].Open.shift(1)\n",
    " \n",
    "Predictors[\"open_incr\"] = CNHI[\"data\"].Open - CNHI[\"data\"].Open.shift(1)\n",
    " \n",
    "Predictors[\"open\"] = CNHI[\"data\"].Open\n",
    " \n",
    "\n",
    "Predictors = Predictors.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.DataFrame({\"value\":Predictors.sma2.shift(-1) - Predictors.sma2}).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.3. Create training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.merge(Predictors, target,left_index=True,right_index=True)[Predictors.columns]\n",
    "y = pd.merge(Predictors, target,left_index=True,right_index=True)[target.columns]\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = int(X.shape[0] * 0.65)\n",
    " \n",
    "X_train = X.iloc[:train_samples]\n",
    "X_test = X.iloc[train_samples:]\n",
    " \n",
    "y_train = y.iloc[:train_samples]\n",
    "y_test = y.iloc[train_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBinary(val):\n",
    "    if val>0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    " \n",
    "\n",
    "y_test_binary = pd.DataFrame(y_test[\"value\"].apply(getBinary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.4. Build XGB regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = xgb.XGBRegressor(gamma=0.0,n_estimators=150,base_score=0.7,colsample_bytree=1,learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.5. Traing the regressors and check result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbModel = regressor.fit(X_train,y_train.value.apply(getBinary))\n",
    " \n",
    "y_predicted = xgbModel.predict(X_test)\n",
    "y_predicted_binary = [1 if yp >=0.5 else 0 for yp in y_predicted] # (y_predicted > 0.5)\n",
    " \n",
    "print ('Model accuracy = %.3f' % accuracy_score(y_test_binary,y_predicted_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pyplot.figure(figsize=(8,8))\n",
    "pyplot.xticks(rotation='vertical')\n",
    "pyplot.bar([i for i in range(len(xgbModel.feature_importances_))], xgbModel.feature_importances_.tolist(), tick_label=X_test.columns)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://tradingsim.com/blog/simple-moving-average/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Other ways to predict prices\n",
    "\n",
    "1. CNN\n",
    "2. Trend - using simple NN or LR to predict whether the trend will be up or down\n",
    "3. RL - algo that trains itself how to trade https://hackernoon.com/the-self-learning-quant-d3329fcc9915\n",
    "4. NLP - for fundamental analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next step: Backtest \n",
    "\n",
    "https://www.quantopian.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "nbpresent": {
   "slides": {
    "2f6752be-0ed8-440b-b3f4-97a45f225158": {
     "id": "2f6752be-0ed8-440b-b3f4-97a45f225158",
     "prev": null,
     "regions": {
      "f2051422-9c14-45a1-995a-512084e6a5b5": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "content": {
        "cell": "01ac3bbb-1083-4fd8-b71a-9ceb84cd8a04",
        "part": "whole"
       },
       "id": "f2051422-9c14-45a1-995a-512084e6a5b5"
      }
     }
    }
   },
   "themes": {
    "default": "5126d5a7-8267-498f-909d-b3c16b14aedf",
    "theme": {
     "5126d5a7-8267-498f-909d-b3c16b14aedf": {
      "backgrounds": {
       "dc7afa04-bf90-40b1-82a5-726e3cff5267": {
        "background-color": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "id": "dc7afa04-bf90-40b1-82a5-726e3cff5267"
       }
      },
      "id": "5126d5a7-8267-498f-909d-b3c16b14aedf",
      "palette": {
       "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
        "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "rgb": [
         252,
         252,
         252
        ]
       },
       "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
        "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "rgb": [
         68,
         68,
         68
        ]
       },
       "50f92c45-a630-455b-aec3-788680ec7410": {
        "id": "50f92c45-a630-455b-aec3-788680ec7410",
        "rgb": [
         197,
         226,
         245
        ]
       },
       "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
        "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "rgb": [
         43,
         126,
         184
        ]
       },
       "efa7f048-9acb-414c-8b04-a26811511a21": {
        "id": "efa7f048-9acb-414c-8b04-a26811511a21",
        "rgb": [
         25.118061674008803,
         73.60176211453744,
         107.4819383259912
        ]
       }
      },
      "rules": {
       "a": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c"
       },
       "blockquote": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3
       },
       "code": {
        "font-family": "Anonymous Pro"
       },
       "h1": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 8
       },
       "h2": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 6
       },
       "h3": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-family": "Lato",
        "font-size": 5.5
       },
       "h4": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 5
       },
       "h5": {
        "font-family": "Lato"
       },
       "h6": {
        "font-family": "Lato"
       },
       "h7": {
        "font-family": "Lato"
       },
       "li": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3.25
       },
       "pre": {
        "font-family": "Anonymous Pro",
        "font-size": 4
       }
      },
      "text-base": {
       "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
       "font-family": "Lato",
       "font-size": 4
      }
     }
    }
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
